# Faiss indekslerini temizleme ve hafÄ±zayÄ± boÅŸaltma iÃ§in zamanlama
def clean_faiss_indexes_periodically():
    """DÃ¼zenli aralÄ±klarla Faiss indekslerini sÄ±fÄ±rla ve hafÄ±zayÄ± temizle"""
    global faiss_indexes, cached_features, cached_filenames, cached_metadata
    
    # Belirli bir sÃ¼re geÃ§tikten sonra temizlik yap
    print("ğŸ•“ PlanlÄ± Faiss indeks temizliÄŸi baÅŸlatÄ±lÄ±yor")
    
    # Faiss indekslerini sÄ±fÄ±rla
    faiss_indexes = {}
    
    # Ã–ncelikle Ã¶zellik vektÃ¶rleri dÄ±ÅŸÄ±ndaki Ã¶nbellekleri sÄ±fÄ±rla
    cached_filenames = {}
    cached_metadata = None
    
    # Bellek toplama iÅŸlemini manuel olarak baÅŸlat
    gc.collect()
    
    # Bir sonraki planlÄ± temizliÄŸi zamanla (4 saat sonra)
    timer = threading.Timer(4 * 60 * 60, clean_faiss_indexes_periodically)  # 4 saat
    timer.daemon = True  # Program kapanÄ±rken thread'in kapanmasÄ±nÄ± saÄŸla
    timer.start()# app.py
# OluÅŸturulma: 2025-04-19
# GÃ¼ncelleme: 2025-04-29 (SQLite VeritabanÄ± Entegrasyonu)
# AÃ§Ä±klama:
# Bu Flask uygulamasÄ±, Power User (PU) arayÃ¼zÃ¼yle etkileÅŸime girer.
# KullanÄ±cÄ±lara metadata gÃ¼ncelleme, model eÄŸitimi ve feedback sistemlerini yÃ¶netme imkÃ¢nÄ± saÄŸlar.
# Åu an aktif olan endpointler:
# - "/"              â†’ Ana yÃ¶nlendirme (ileride dashboard'a baÄŸlanabilir)
# - "/train"         â†’ EÄŸitim arayÃ¼zÃ¼ (train.html)
# - "/train-model"   â†’ POST ile eÄŸitim baÅŸlatma
# - "/check-updates" â†’ JSON & gÃ¶rsel gÃ¼ncellemelerini kontrol eder
# - "/find-similar"  â†’ Benzer desen gÃ¶rsellerini getirir
# - "/realImages/<path:filename>" â†’ GerÃ§ek gÃ¶rselleri sunar
# - "/create-cluster" â†’ SeÃ§ilen gÃ¶rsellerle yeni cluster oluÅŸturur

from flask import Flask, render_template, request, jsonify, send_from_directory
from train_handlers import setup_training_routes
import json
import subprocess
import sys
import os
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
import shutil
from datetime import datetime
import random
import traceback
import faiss  # Faiss kÃ¼tÃ¼phanesini ekledik
import colorsys  # Renk dÃ¶nÃ¼ÅŸÃ¼mleri iÃ§in eklendi
import cv2
import threading
import gc
import time  # time modÃ¼lÃ¼nÃ¼ eksikti
import sqlite3
import db_utils  # SQLite iÅŸlemleri iÃ§in yardÄ±mcÄ± modÃ¼l

# SÃ¼rÃ¼m bilgisi
APP_VERSION = "1.2.0"
APP_LAST_UPDATED = "2025-04-28"

app = Flask(__name__)

# Faiss indekslerini saklamak iÃ§in global deÄŸiÅŸken
faiss_indexes = {}

# Ã–zellik vektÃ¶rlerini ve dosya isimlerini saklamak iÃ§in global deÄŸiÅŸkenler
cached_features = {}
cached_filenames = {}
cached_metadata = None

# Faiss indeksi oluÅŸturma ve alÄ±m iÃ§in yardÄ±mcÄ± fonksiyon
def get_faiss_index(model_type):
    """Belirtilen model tipi iÃ§in Faiss indeksi oluÅŸturur veya Ã¶nbellekten alÄ±r"""
    # Ã–nbellekte varsa dÃ¶ndÃ¼r
    if model_type in faiss_indexes:
        return faiss_indexes[model_type]
    
    # Ã–nbellekte yoksa yÃ¼kle
    feature_path = f"image_features/{model_type}_features.npy"
    if not os.path.exists(feature_path):
        return None
    
    # Ã–zellik vektÃ¶rlerini yÃ¼kle (Ã¶nbellekte yoksa)
    if model_type not in cached_features:
        features = np.load(feature_path)
        cached_features[model_type] = features.astype(np.float32)
    else:
        features = cached_features[model_type]
    
    # Faiss indeksi oluÅŸtur
    d = features.shape[1]  # VektÃ¶r boyutu
    
    # GPU kontrolÃ¼
    use_gpu = faiss.get_num_gpus() > 0
    
    if model_type in ["pattern", "color+pattern"] or model_type.startswith("pattern"):
        # KosinÃ¼s benzerliÄŸi iÃ§in L2 normalleÅŸtirme
        normalized_features = features.copy()
        faiss.normalize_L2(normalized_features)
        
        if use_gpu:
            # GPU ile IP (Inner Product) indeksi
            print(f"ğŸš€ {model_type} iÃ§in GPU ile Faiss IP indeksi oluÅŸturuluyor")
            flat_config = faiss.GpuIndexFlatConfig()
            flat_config.device = 0
            res = faiss.StandardGpuResources()
            index = faiss.GpuIndexFlatIP(res, d, flat_config)
        else:
            # CPU ile IP indeksi
            print(f"ğŸ“Š {model_type} iÃ§in CPU ile Faiss IP indeksi oluÅŸturuluyor")
            index = faiss.IndexFlatIP(d)
        
        # Normalize edilmiÅŸ vektÃ¶rleri ekle
        index.add(normalized_features)
    else:
        # Ã–klid mesafesi iÃ§in L2 indeksi
        if use_gpu:
            print(f"ğŸš€ {model_type} iÃ§in GPU ile Faiss L2 indeksi oluÅŸturuluyor")
            flat_config = faiss.GpuIndexFlatConfig()
            flat_config.device = 0
            res = faiss.StandardGpuResources()
            index = faiss.GpuIndexFlatL2(res, d, flat_config)
        else:
            print(f"ğŸ“Š {model_type} iÃ§in CPU ile Faiss L2 indeksi oluÅŸturuluyor")
            index = faiss.IndexFlatL2(d)
        
        # VektÃ¶rleri ekle
        index.add(features)
    
    # Ä°ndeksi Ã¶nbelleÄŸe al
    faiss_indexes[model_type] = index
    
    return index

# Renk benzerliÄŸi hesaplayan fonksiyon
def calculate_color_similarity(color1, color2):
    """Ä°ki RGB rengi arasÄ±ndaki benzerliÄŸi hesaplar (0-1 arasÄ±)"""
    # Euclidean mesafe tabanlÄ± basit benzerlik
    color1 = np.array(color1)
    color2 = np.array(color2)
    
    # 3D renk uzayÄ±nda mesafe hesapla
    distance = np.sqrt(np.sum((color1 - color2)**2))
    
    # Maksimum mesafe 441.67 (255, 255, 255 ile 0, 0, 0 arasÄ±)
    max_distance = np.sqrt(3 * 255**2)
    
    # Mesafeyi benzerliÄŸe dÃ¶nÃ¼ÅŸtÃ¼r (0-1 arasÄ±)
    similarity = 1.0 - (distance / max_distance)
    
    return similarity

# Uyumlu renkleri bulan fonksiyon
def find_harmonious_colors(reference_color, harmony_type="complementary"):
    """Bir referans renge uyumlu renkleri bulan fonksiyon
    
    Args:
        reference_color: RGB formatÄ±nda referans renk [r, g, b]
        harmony_type: Uyum tipi ('complementary', 'analogous', 'triadic', 'split_complementary', 'monochromatic')
    
    Returns:
        Uyumlu renklerin listesi
    """
    # RGB'yi HSV'ye dÃ¶nÃ¼ÅŸtÃ¼r (renk teorisi iÅŸlemleri HSV'de daha kolay)
    hsv_color = colorsys.rgb_to_hsv(reference_color[0]/255, reference_color[1]/255, reference_color[2]/255)
    h, s, v = hsv_color
    
    harmonious_colors = []
    
    if harmony_type == "complementary":
        # TamamlayÄ±cÄ± renk (renk Ã§emberinde 180 derece karÅŸÄ±)
        complementary_h = (h + 0.5) % 1.0
        complementary_rgb = colorsys.hsv_to_rgb(complementary_h, s, v)
        harmonious_colors.append([int(c * 255) for c in complementary_rgb])
    
    elif harmony_type == "analogous":
        # Analog renkler (renk Ã§emberinde +/- 30 derece)
        for angle in [-30, 30]:
            analog_h = (h + angle/360) % 1.0
            analog_rgb = colorsys.hsv_to_rgb(analog_h, s, v)
            harmonious_colors.append([int(c * 255) for c in analog_rgb])
    
    elif harmony_type == "triadic":
        # Triadik renkler (renk Ã§emberinde +/- 120 derece)
        for angle in [120, 240]:
            triad_h = (h + angle/360) % 1.0
            triad_rgb = colorsys.hsv_to_rgb(triad_h, s, v)
            harmonious_colors.append([int(c * 255) for c in triad_rgb])
    
    elif harmony_type == "split_complementary":
        # Split complementary (tamamlayÄ±cÄ± rengin her iki yanÄ±ndan 30'ar derece)
        comp_h = (h + 0.5) % 1.0
        for angle in [-30, 30]:
            split_h = (comp_h + angle/360) % 1.0
            split_rgb = colorsys.hsv_to_rgb(split_h, s, v)
            harmonious_colors.append([int(c * 255) for c in split_rgb])
    
    elif harmony_type == "monochromatic":
        # Monokromatik renkler (aynÄ± rengin farklÄ± ton ve doygunluklarÄ±)
        # Daha aÃ§Ä±k ve daha koyu versiyonlar
        for value_mod in [-0.3, -0.15, 0.15, 0.3]:  # V deÄŸeri deÄŸiÅŸimleri
            new_v = max(0.1, min(0.9, v + value_mod))  # SÄ±nÄ±rlar iÃ§inde tut
            mono_rgb = colorsys.hsv_to_rgb(h, s, new_v)
            harmonious_colors.append([int(c * 255) for c in mono_rgb])
    
    return harmonious_colors

# YardÄ±mcÄ± fonksiyon: Filtre uygulamalarÄ± iÃ§in izin verilen indeksleri oluÅŸtur
def get_allowed_indices(filters, filenames, metadata, model_type, version):
    """Filtre parametrelerine gÃ¶re izin verilen indeksleri dÃ¶ndÃ¼rÃ¼r"""
    allowed_indices = []
    
    if not filters:
        # Filtre yoksa ve versiyon v1 deÄŸilse, versiyon bazlÄ± filtreleme yap
        if version != "v1":
            model_data = get_model_data(model_type)
            version_data = model_data.get("versions", {}).get(version, {})
            version_clusters = version_data.get("clusters", {})
            
            # TÃ¼m clusterlar'daki gÃ¶rselleri topla
            version_images = set()
            for cluster_data in version_clusters.values():
                version_images.update(cluster_data.get("images", []))
            
            # Sadece bu versiyondaki gÃ¶rselleri dahil et
            allowed_indices = [i for i, fname in enumerate(filenames) if fname in version_images]
            print(f"â„¹ï¸ Versiyon filtreleme: {version} versiyonunda {len(allowed_indices)} gÃ¶rsel bulundu.")
        else:
            allowed_indices = list(range(len(filenames)))
        return allowed_indices

    # Filtreleri iÅŸle
    mix_filters = filters.get("mixFilters", [])
    feature_filters = filters.get("features", [])
    cluster_status = filters.get("cluster", "")
    
    # Model verilerini yÃ¼kle (versiyon bilgisi iÃ§in)
    model_data = get_model_data(model_type)

    for i, fname in enumerate(filenames):
        meta = metadata.get(fname, {})
        feature_map = {f[0]: f[1] for f in meta.get("features", [])}
        cluster = meta.get("cluster")

        match_mix = all(
            mix["min"] <= feature_map.get(mix["type"], 0) <= mix["max"]
            for mix in mix_filters
        )

        match_feature = all(f in feature_map for f in feature_filters)

        # Versiyon bazlÄ± cluster kontrolÃ¼
        match_cluster = True
        if cluster_status == "clustered" and not cluster:
            match_cluster = False
        elif cluster_status == "unclustered" and cluster:
            match_cluster = False
            
        # Versiyon kontrolÃ¼: EÄŸer metadata'da cluster bilgisi varsa
        # ve bu cluster belirtilen model ve versiyonda yer alÄ±yorsa, kullan
        if version != "v1" and cluster:  # v1 tÃ¼m gÃ¶rseller iÃ§in varsayÄ±lan
            # Model verisinde bu cluster var mÄ± kontrol et
            version_data = model_data.get("versions", {}).get(version, {})
            version_clusters = version_data.get("clusters", {})
            
            cluster_exists = False
            for cluster_name, cluster_data in version_clusters.items():
                if fname in cluster_data.get("images", []):
                    cluster_exists = True
                    break
            
            # Bu gÃ¶rsel belirtilen versiyonda deÄŸilse ve clustered filtresi varsa, filtreleme yap
            if cluster_status == "clustered" and not cluster_exists:
                match_cluster = False

        if match_mix and match_feature and match_cluster:
            allowed_indices.append(i)
            
    return allowed_indices
setup_training_routes(app)

# Model JSON yÃ¶netimi iÃ§in yardÄ±mcÄ± fonksiyonlar
def get_model_data(model):
    """Model JSON verisini okur"""
    model_path = os.path.join("exported_clusters", model, "versions.json")
    
    if not os.path.exists(model_path):
        # Yeni model dosyasÄ± oluÅŸtur
        data = {
            "current_version": "v1",
            "versions": {}
        }
        # Model klasÃ¶rÃ¼nÃ¼ oluÅŸtur
        os.makedirs(os.path.join("exported_clusters", model), exist_ok=True)
        with open(model_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        return data
        
    try:
        with open(model_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError:
        # Bozuksa yeniden oluÅŸtur
        return {
            "current_version": "v1",
            "versions": {}
        }

def save_model_data(model, data):
    """Model JSON verisini kaydeder"""
    model_path = os.path.join("exported_clusters", model, "versions.json")
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    
    with open(model_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    return True

def get_next_version(model):
    """Bir sonraki versiyon adÄ±nÄ± dÃ¶ndÃ¼rÃ¼r (v1, v2, ...)"""
    data = get_model_data(model)
    versions = data["versions"].keys()
    
    if not versions:
        return "v1"
        
    # v1, v2, ... formatÄ±ndaki versiyonlarÄ± bul
    v_numbers = [int(v[1:]) for v in versions if v.startswith("v") and v[1:].isdigit()]
    
    if not v_numbers:
        return "v1"
        
    return f"v{max(v_numbers) + 1}"

# Metadata gÃ¼ncelleme yardÄ±mcÄ± fonksiyonu
def update_metadata(filename, updates):
    """GÃ¶rsel metadatasÄ±nÄ± gÃ¼nceller"""
    metadata_path = "image_metadata_map.json"
    try:
        with open(metadata_path, "r", encoding="utf-8") as f:
            metadata = json.load(f)
        
        if filename in metadata:
            metadata[filename].update(updates)
        else:
            metadata[filename] = updates
            
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
            
        return True
    except Exception as e:
        print(f"Metadata gÃ¼ncelleme hatasÄ±: {str(e)}")
        return False

@app.route("/")
def home():
    return "<h2>PU Ana Panel </h2><p>/train ile model eÄŸit, /check-updates ile gÃ¼ncelle kontrol et.</p>"


@app.route("/train")
def train():
    return render_template("train.html")

@app.route("/pu")
def pu_screen():
    return render_template("pu.html")

@app.route("/train-model", methods=["POST"])
def train_model_route():
    data = request.get_json()
    with open("train_config.json", "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)

    # EÄŸitim klasÃ¶rÃ¼ yoksa oluÅŸtur
    cluster_path = os.path.join("exported_clusters", data["model_type"])
    os.makedirs(cluster_path, exist_ok=True)

    python_exe = sys.executable
    env = os.environ.copy()
    env["PYTHONIOENCODING"] = "utf-8"

    try:
        result = subprocess.run([python_exe, "train_model.py"], capture_output=True, text=True, check=True, env=env)
        print(result.stdout)

        # EÄŸitim sonrasÄ± Ã¶zet satÄ±rÄ± bul (âœ… EÄŸitim Ã–zeti: ...)
        summary_line = ""
        for line in result.stdout.splitlines():
            if line.startswith("âœ… EÄŸitim Ã–zeti"):
                summary_line = line
                break

        status_msg = summary_line if summary_line else "âœ… EÄŸitim baÅŸarÄ±yla tamamlandÄ±."
        return jsonify({"status": status_msg})
    except subprocess.CalledProcessError as e:
        return jsonify({"status": f"âŒ EÄŸitim sÄ±rasÄ±nda hata: {e.stderr}"})

@app.route('/image_metadata_map.json')
def serve_metadata_map():
    return send_from_directory('.', 'image_metadata_map.json')

@app.route('/realImages/<path:filename>')
def serve_real_image(filename):
    return send_from_directory('realImages', filename)

@app.route("/check-updates")
def check_updates():
    try:
        from check_updates import check_for_updates
        changes = check_for_updates()
        
        if changes.get("has_changes", False):
            print(f"ğŸ” DeÄŸiÅŸiklik algÄ±landÄ±: {changes.get('summary', {})}")
            return jsonify({
                "status": "changes_detected",
                "changes": changes,
                "message": changes.get("notification", {}).get("message", "DeÄŸiÅŸiklikler algÄ±landÄ±.")
            })
        else:
            return jsonify({
                "status": "up_to_date",
                "message": changes.get("message", "âœ… DeÄŸiÅŸiklik yok, metadata gÃ¼ncel.")
            })
    except Exception as ex:
        traceback.print_exc()
        return jsonify({
            "status": "error",
            "message": f"âŒ Hata oluÅŸtu: {str(ex)}"
        })
@app.route('/thumbnails/<path:filename>')
def serve_thumbnail(filename):
    return send_from_directory('thumbnails', filename)

@app.route("/clusters/<model>/<version>/data")
def serve_cluster_data(model, version):
    """Model versiyon bilgilerini dÃ¶ndÃ¼rÃ¼r"""
    model_data = get_model_data(model)
    
    # EÄŸer versiyon yoksa hata dÃ¶ndÃ¼r
    if version not in model_data.get("versions", {}):
        return jsonify({"error": "Versiyon bulunamadÄ±"}), 404
    
    # Versiyon iÃ§eriklerini kontrol et
    version_data = model_data["versions"][version]
    clusters = version_data.get("clusters", {})
    cluster_count = len(clusters)
    total_images = 0
    
    # Her cluster'daki gÃ¶rsel sayÄ±sÄ±nÄ± topla
    for cluster_name, cluster_data in clusters.items():
        total_images += len(cluster_data.get("images", []))
    
    print(f"\n[DEBUG] Model: {model}, Version: {version}")
    print(f"[DEBUG] Cluster count: {cluster_count}")
    print(f"[DEBUG] Total images: {total_images}")
    print(f"[DEBUG] Clusters: {list(clusters.keys())}\n")
    
    # Sadece istenen versiyonun verisini dÃ¶ndÃ¼r
    return jsonify({
        "version": version,
        "data": model_data["versions"][version],
        "stats": {
            "cluster_count": cluster_count,
            "total_images": total_images
        }
    })

@app.route("/find-similar", methods=["GET", "POST"])
def find_similar():
    """SQLite veritabanÄ± kullanan benzerlik arama endpoint'i"""
    start_time = datetime.now()  # Performans Ã¶lÃ§Ã¼mÃ¼ iÃ§in baÅŸlangÄ±Ã§ zamanÄ±
    
    filters = request.get_json() if request.method == "POST" else None
    filename = request.args.get("filename")
    model = request.args.get("model", "pattern")
    version = request.args.get("version", "v1")
    topN = int(request.args.get("topN", 100))
    metric = request.args.get("metric", "cosine")
    
    print(f"ğŸ“Š Benzer gÃ¶rsel arama: model={model}, version={version}, metric={metric}")

    # db_utils modÃ¼lÃ¼ aracÄ±lÄ±ÄŸÄ±yla SQLite tabanlÄ± arama yap
    # Fabric bilgilerini kontrol et
    fabric = db_utils.get_fabric_by_filename(filename)
    if not fabric:
        print(f"âš ï¸ KumaÅŸ bulunamadÄ±: {filename}")
        # Eski yÃ¶nteme geri dÃ¶n (uyumluluk iÃ§in)
        try:
            # Metadata dosyasÄ±nÄ± kontrol et
            metadata_path = "image_metadata_map.json"
            if os.path.exists(metadata_path):
                with open(metadata_path, "r", encoding="utf-8") as f:
                    metadata = json.load(f)
                if filename not in metadata:
                    return jsonify([])
        except:
            return jsonify([])

    # Benzer gÃ¶rselleri bul
    results = db_utils.find_similar_images(
        filename=filename,
        model_type=model,
        version=version,
        topN=topN,
        metric=metric,
        filters=filters
    )
    
    # SonuÃ§ yoksa ve hala eski JSON yapÄ±sÄ± varsa, uyumluluk modu iÃ§in eski yÃ¶ntemi dene
    if not results:
        feature_path = f"image_features/{model}_features.npy"
        index_path = f"image_features/{model}_filenames.json"
        
        if os.path.exists(feature_path) and os.path.exists(index_path) and cached_metadata is not None:
            print(f"âš ï¸ SQLite sonuÃ§ yok, eski yÃ¶ntem deneniyor: {filename}")
            
            # Eskiden kullanÄ±lan JSON-tabanlÄ± arama fonksiyonuna yÃ¶nlendir
            try:
                # Ã–nbellekte yoksa dosya isimlerini yÃ¼kle
                if model not in cached_filenames:
                    with open(index_path, "r", encoding="utf-8") as f:
                        cached_filenames[model] = json.load(f)
                filenames = cached_filenames[model]
                
                # Faiss indeksini al
                index = get_faiss_index(model)
                if index and filename in filenames:
                    # Burada eski find_similar iÅŸlevi Ã§aÄŸrÄ±labilir
                    # Ancak bu geÃ§iÅŸ aÅŸamasÄ±nda kullanÄ±lacak geÃ§ici bir Ã§Ã¶zÃ¼m
                    print(f"âš ï¸ Eski arama yÃ¶ntemi kullanÄ±lÄ±yor")
            except Exception as e:
                print(f"âŒ Eski yÃ¶ntem hatasÄ±: {str(e)}")
    
    # SonuÃ§larÄ±n ilk elemanÄ±, aranÄ±lan gÃ¶rsel olmalÄ± (tam eÅŸleÅŸme)
    # EÄŸer yoksa ekle
    self_exists = any(r["filename"] == filename for r in results)
    
    if not self_exists:
        # db_utils ile fabric bilgilerini al
        reference_fabric = db_utils.get_fabric_by_filename(filename)
        
        if reference_fabric:
            # En baÅŸa kendisini ekle
            results.insert(0, {
                "filename": filename,
                "design": reference_fabric.get("design"),
                "season": reference_fabric.get("season"),
                "quality": reference_fabric.get("quality"),
                "features": reference_fabric.get("features", []),
                "cluster": None,  # Cluster bilgisi sonra eklenecek
                "version": version,
                "similarity": 1.0  # Tam eÅŸleÅŸme her zaman 1.0
            })
    
    # Performans Ã¶lÃ§Ã¼mÃ¼
    end_time = datetime.now()
    duration_ms = (end_time - start_time).total_seconds() * 1000
    print(f"â±ï¸ Benzerlik aramasÄ± {duration_ms:.1f} ms'de tamamlandÄ± ({len(results)} sonuÃ§)")
    
    return jsonify(results)


@app.route("/update-version-comment", methods=["POST"])
def update_version_comment():
    data = request.get_json()
    model = data.get("model")
    version = data.get("version")
    comment = data.get("version_comment", "")

    if not model or not version:
        return jsonify({"status": "error", "message": "Model ve versiyon belirtilmelidir."})

    try:
        model_data = get_model_data(model)
        
        if version not in model_data.get("versions", {}):
            return jsonify({"status": "error", "message": "Belirtilen versiyon bulunamadÄ±."})
        
        model_data["versions"][version]["comment"] = comment
        save_model_data(model, model_data)

        return jsonify({"status": "ok"})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)})


@app.route("/create-cluster", methods=["POST"])
def create_cluster():
    """SQLite entegrasyonlu kÃ¼me oluÅŸturma endpoint'i"""
    data = request.get_json()
    model = data.get("model")
    version = data.get("version")
    filenames = data.get("filenames", [])

    if not model or not version or not filenames:
        return jsonify({"status": "error", "message": "Model, versiyon ve gÃ¶rsel listesi gerekli."})

    try:
        # Versiyon kontrolÃ¼ - SQLite kullanarak
        conn = db_utils.create_connection()
        if not conn:
            return jsonify({"status": "error", "message": "VeritabanÄ±na baÄŸlanÄ±lamadÄ±."})
        
        cursor = conn.cursor()
        
        # Versiyon ID'sini bul
        cursor.execute("""
            SELECT id FROM model_versions
            WHERE model_type = ? AND version_name = ?
        """, (model, version))
        
        version_row = cursor.fetchone()
        version_id = None
        
        # EÄŸer versiyon yoksa oluÅŸtur
        if not version_row:
            print(f"âœ… Yeni versiyon oluÅŸturuluyor: {model}/{version}")
            # Versiyon yoksa oluÅŸtur
            cursor.execute("""
                INSERT INTO model_versions (model_type, version_name, creation_date, parameters)
                VALUES (?, ?, ?, ?)
            """, (
                model,
                version,
                datetime.now().isoformat(),
                json.dumps({"algorithm": "manual"})
            ))
            version_id = cursor.lastrowid
        else:
            version_id = version_row['id']
        
        # GÃ¶rsel ID'lerini al
        placeholders = ','.join(['?' for _ in filenames])
        cursor.execute(f"""
            SELECT id, filename FROM fabrics 
            WHERE filename IN ({placeholders})
        """, filenames)
        
        fabric_ids = {row['filename']: row['id'] for row in cursor.fetchall()}
        
        # EÄŸer bazÄ± gÃ¶rseller bulunamadÄ±ysa uyar
        missing_files = [f for f in filenames if f not in fabric_ids]
        if missing_files:
            print(f"âš ï¸ Bu dosyalar veritabanÄ±nda bulunamadÄ±: {missing_files}")
        
        # Bu gÃ¶rsellerin mevcut kÃ¼melerden Ã§Ä±karÄ±lmasÄ±
        # Ã–nce bu versiyondaki tÃ¼m kÃ¼meleri bul
        cursor.execute("""
            SELECT c.id, c.cluster_number, c.representative_id 
            FROM clusters c WHERE c.version_id = ?
        """, (version_id,))
        
        clusters = cursor.fetchall()
        
        for cluster in clusters:
            # Bu kÃ¼mede bulunan gÃ¶rselleri bul
            cursor.execute("""
                SELECT fabric_id FROM fabric_clusters 
                WHERE cluster_id = ? AND fabric_id IN ({})
            """.format(','.join(['?' for _ in fabric_ids.values()])), 
                [cluster['id']] + list(fabric_ids.values()))
            
            to_remove = cursor.fetchall()
            
            # Bu gÃ¶rselleri kÃ¼meden Ã§Ä±kar
            for row in to_remove:
                cursor.execute("""
                    DELETE FROM fabric_clusters 
                    WHERE cluster_id = ? AND fabric_id = ?
                """, (cluster['id'], row['fabric_id']))
            
            # EÄŸer temsil eden gÃ¶rsel Ã§Ä±karÄ±ldÄ±ysa
            if cluster['representative_id'] in fabric_ids.values():
                # Geriye kalan herhangi bir Ã¼ye var mÄ±?
                cursor.execute("""
                    SELECT fabric_id FROM fabric_clusters 
                    WHERE cluster_id = ? LIMIT 1
                """, (cluster['id'],))
                
                remaining = cursor.fetchone()
                
                if remaining:
                    # Yeni temsil eden gÃ¶rsel seÃ§
                    cursor.execute("""
                        UPDATE clusters SET representative_id = ? 
                        WHERE id = ?
                    """, (remaining['fabric_id'], cluster['id']))
                else:
                    # KÃ¼me tamamen boÅŸaldÄ±, sil
                    cursor.execute("""
                        DELETE FROM clusters 
                        WHERE id = ?
                    """, (cluster['id'],))
        
        # Yeni kÃ¼me numarasÄ± iÃ§in mevcut en yÃ¼ksek numarayÄ± bul
        cursor.execute("""
            SELECT MAX(cluster_number) as max_num FROM clusters 
            WHERE version_id = ?
        """, (version_id,))
        
        max_num = cursor.fetchone()
        next_id = 1
        if max_num and max_num['max_num'] is not None:
            next_id = max_num['max_num'] + 1
        
        # Temsil eden gÃ¶rseli seÃ§ (ilk gÃ¶rsel)
        representative = filenames[0]
        if representative in fabric_ids:
            # Yeni kÃ¼me oluÅŸtur
            cursor.execute("""
                INSERT INTO clusters (version_id, cluster_number, representative_id)
                VALUES (?, ?, ?)
            """, (version_id, next_id, fabric_ids[representative]))
            
            cluster_id = cursor.lastrowid
            
            # KÃ¼meye gÃ¶rselleri ekle
            for filename in filenames:
                if filename in fabric_ids:
                    cursor.execute("""
                        INSERT INTO fabric_clusters (fabric_id, cluster_id)
                        VALUES (?, ?)
                    """, (fabric_ids[filename], cluster_id))
            
            # DeÄŸiÅŸiklikleri kaydet
            conn.commit()
            
            new_cluster_name = f"cluster-{next_id}"
            
            # ESKÄ° YÃ–NTEM (UYUMLULUK Ä°Ã‡Ä°N): JSON dosyalarÄ±nÄ± da gÃ¼ncelle
            try:
                # Model verisini al
                model_data = get_model_data(model)
                
                # Versiyon kontrolÃ¼
                if version not in model_data["versions"]:
                    model_data["versions"][version] = {
                        "created_at": datetime.now().isoformat(),
                        "comment": f"{model} modeli {version} versiyonu",
                        "algorithm": "manual",
                        "parameters": {},
                        "clusters": {}
                    }
                
                # Eski cluster'lardan seÃ§ili gÃ¶rselleri Ã§Ä±kar
                version_data = model_data["versions"][version]
                for cluster_name, cluster_data in list(version_data["clusters"].items()):
                    cluster_data["images"] = [img for img in cluster_data["images"] if img not in filenames]
                    
                    if not cluster_data["images"] or cluster_data["representative"] in filenames:
                        if cluster_data["images"]:
                            cluster_data["representative"] = cluster_data["images"][0]
                        else:
                            del version_data["clusters"][cluster_name]
                
                # Thumbnail klasÃ¶rÃ¼nÃ¼ oluÅŸtur (eÄŸer yoksa)
                thumbnail_path = os.path.join("exported_clusters", model, version, "thumbnails")
                os.makedirs(thumbnail_path, exist_ok=True)
                
                # GÃ¶rsellerin thumbnail'larÄ±nÄ± kopyala
                for fname in filenames:
                    src_thumb = os.path.join("thumbnails", fname)
                    dst_thumb = os.path.join(thumbnail_path, fname)
                    
                    if os.path.exists(src_thumb):
                        shutil.copy2(src_thumb, dst_thumb)
                        
                    # Metadatada gÃ¼ncelle (eski yÃ¶ntem)
                    update_metadata(fname, {"cluster": new_cluster_name})
                
                # Yeni cluster'a gÃ¶rselleri ekle
                version_data["clusters"][new_cluster_name] = {
                    "representative": representative,
                    "images": filenames,
                    "comment": ""
                }
                
                # Model verisini kaydet
                save_model_data(model, model_data)
            except Exception as e:
                print(f"âš ï¸ JSON dosya gÃ¼ncellemesi baÅŸarÄ±sÄ±z: {str(e)}")
            
            return jsonify({"status": "ok", "new_cluster": new_cluster_name})
        else:
            # Temsil eden gÃ¶rsel bulunamadÄ±
            conn.rollback()
            return jsonify({"status": "error", "message": "Temsil eden gÃ¶rsel bulunamadÄ±."})

    except Exception as e:
        import traceback
        print(f"Cluster oluÅŸturma hatasÄ±: {str(e)}")
        print(traceback.format_exc())
        
        # Hata durumunda iÅŸlemi geri al
        if 'conn' in locals():
            conn.rollback()
        
        return jsonify({"status": "error", "message": str(e)})
    finally:
        # BaÄŸlantÄ±yÄ± kapat
        if 'conn' in locals() and conn:
            conn.close()

# --- Feedback kaydÄ± alma endpointi (SQLite) ---
@app.route("/submit-feedback", methods=["POST"])
def submit_feedback():
    feedback_data = request.get_json()
    print("ğŸ“© Feedback alÄ±ndÄ±:", feedback_data)

    # Gerekli verileri al
    anchor = feedback_data.get("anchor")
    output = feedback_data.get("output")
    model = feedback_data.get("model")
    version = feedback_data.get("version")
    rating = feedback_data.get("feedback")

    # EÄŸer rating yoksa, bu bir iptal iÅŸlemidir
    if rating is None:
        # Eski yÃ¶ntem: JSON dosyasÄ±nÄ± gÃ¼ncelle (uyumluluk iÃ§in)
        feedback_file = "feedback_log.json"
        if os.path.exists(feedback_file):
            try:
                with open(feedback_file, "r", encoding="utf-8") as f:
                    feedback_list = json.load(f)
                
                # Ä°lgili feedback'i Ã§Ä±kar
                feedback_list = [f for f in feedback_list if not (
                    f.get("anchor") == anchor and
                    f.get("output") == output and
                    f.get("model") == model and
                    f.get("version") == version
                )]
                
                # GÃ¼ncellenmiÅŸ listeyi kaydet
                with open(feedback_file, "w", encoding="utf-8") as f:
                    json.dump(feedback_list, f, indent=2, ensure_ascii=False)
            except Exception as e:
                print(f"âš ï¸ Feedback JSON gÃ¼ncellemesi hatasÄ±: {str(e)}")

        # VeritabanÄ±ndan da sil (ileride eklenecek)
        # db_utils.delete_feedback(anchor, output, model, version)
        return jsonify({"status": "ok", "message": "Feedback silindi"})

    # SQLite veritabanÄ±na ekle
    success = db_utils.add_feedback(
        anchor_filename=anchor,
        output_filename=output,
        model_type=model,
        version=version,
        rating=rating
    )

    # Eski yÃ¶ntem: JSON dosyasÄ±na da ekle (uyumluluk iÃ§in)
    feedback_file = "feedback_log.json"
    feedback_list = []

    if os.path.exists(feedback_file):
        with open(feedback_file, "r", encoding="utf-8") as f:
            try:
                feedback_list = json.load(f)
            except json.JSONDecodeError:
                feedback_list = []

    # AynÄ± anchor-output-model-version varsa Ã§Ä±kar
    feedback_list = [f for f in feedback_list if not (
        f.get("anchor") == anchor and
        f.get("output") == output and
        f.get("model") == model and
        f.get("version") == version
    )]

    # Yeni feedback'i ekle
    feedback_list.append(feedback_data)

    with open(feedback_file, "w", encoding="utf-8") as f:
        json.dump(feedback_list, f, indent=2, ensure_ascii=False)

    if success:
        return jsonify({"status": "ok", "message": "Feedback kaydedildi"})
    else:
        return jsonify({"status": "error", "message": "Feedback kaydedilirken bir hata oluÅŸtu"})

@app.route("/init-model-version", methods=["POST"])
def init_model_version():
    """Yeni bir model ve versiyon kombinasyonu iÃ§in gereken baÅŸlangÄ±Ã§ yapÄ±sÄ±nÄ± oluÅŸturur"""
    data = request.get_json()
    model = data.get("model")
    version = data.get("version")
    
    if not model or not version:
        return jsonify({"status": "error", "message": "Model ve versiyon belirtilmeli."})
    
    # Model ve versiyon klasÃ¶rlerini oluÅŸtur
    version_path = os.path.join("exported_clusters", model, version)
    os.makedirs(version_path, exist_ok=True)
    
    # BoÅŸ representatives.json dosyasÄ± oluÅŸtur
    rep_data = {
        "representatives": [],
        "clusters": [],
        "version_comment": f"{model} modeli {version} versiyonu",
        "last_updated": datetime.now().isoformat()
    }
    
    rep_path = os.path.join(version_path, "representatives.json")
    with open(rep_path, "w", encoding="utf-8") as f:
        json.dump(rep_data, f, indent=2, ensure_ascii=False)
    
    return jsonify({"status": "ok", "message": f"{model} modeli {version} versiyonu hazÄ±rlandÄ±."})

@app.route("/move-to-cluster", methods=["POST"])
def move_to_cluster():
    data = request.get_json()
    model = data.get("model", "pattern") 
    version = data.get("version", "v1")   
    cluster_name = data.get("cluster")
    images = data.get("images", [])

    if not cluster_name or not images:
        return jsonify({"status": "error", "message": "Eksik bilgi"}), 400

    try:
        # Model verisini al
        model_data = get_model_data(model)
        
        # Versiyon kontrolÃ¼
        if version not in model_data["versions"]:
            return jsonify({"status": "error", "message": "Versiyon bulunamadÄ±"}), 404
            
        version_data = model_data["versions"][version]
        
        # Cluster kontrolÃ¼
        if cluster_name not in version_data["clusters"]:
            return jsonify({"status": "error", "message": "Cluster bulunamadÄ±"}), 404
        
        # Metadata dosyasÄ±nÄ± yÃ¼kle
        metadata_path = "image_metadata_map.json"
        with open(metadata_path, "r", encoding="utf-8") as f:
            metadata = json.load(f)

        moved = []
        
        # GÃ¶rselleri diÄŸer clusterlardan Ã§Ä±kar
        for existing_cluster, cluster_data in list(version_data["clusters"].items()):
            if existing_cluster == cluster_name:
                continue
                
            # GÃ¶rselleri filtrele
            cluster_data["images"] = [img for img in cluster_data["images"] if img not in images]
            
            # Temsil eden gÃ¶rsel taÅŸÄ±ndÄ±ysa, yeni temsil eden seÃ§
            if cluster_data["representative"] in images and cluster_data["images"]:
                cluster_data["representative"] = cluster_data["images"][0]
            
            # EÄŸer cluster boÅŸalmÄ±ÅŸsa ve temsil eden gÃ¶rsel de taÅŸÄ±ndÄ±ysa, cluster'Ä± sil
            if not cluster_data["images"]:
                del version_data["clusters"][existing_cluster]
        
        # GÃ¶rselleri thumbnail olarak kopyala
        thumbnail_path = os.path.join("exported_clusters", model, version, "thumbnails")
        os.makedirs(thumbnail_path, exist_ok=True)
        
        for img in images:
            src_thumb = os.path.join("thumbnails", img)
            dst_thumb = os.path.join(thumbnail_path, img)
            
            if os.path.exists(src_thumb):
                shutil.copy2(src_thumb, dst_thumb)
                moved.append(img)
            else:
                print(f"âš ï¸ Thumbnail bulunamadÄ±: {img}")
            
            # Hedef cluster'a ekle
            if img not in version_data["clusters"][cluster_name]["images"]:
                version_data["clusters"][cluster_name]["images"].append(img)

            # Metadata'da cluster gÃ¼ncelle
            if img in metadata:
                metadata[img]["cluster"] = cluster_name
            else:
                print(f"ğŸ“› Metadata'da {img} bulunamadÄ±")

        # Metadata'yÄ± kaydet
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
            
        # Model verisini kaydet
        save_model_data(model, model_data)

        return jsonify({"status": "ok", "moved": moved, "cluster": cluster_name})
        
    except Exception as e:
        import traceback
        print(f"GÃ¶rsel taÅŸÄ±ma hatasÄ±: {str(e)}")
        print(traceback.format_exc())
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route("/create-new-version", methods=["POST"])
def create_new_version():
    """Yeni bir model versiyonu oluÅŸturur ve clustering yapar"""
    data = request.get_json()
    
    model = data.get("model", "pattern")
    algorithm = data.get("algorithm", "kmeans")
    parameters = data.get("parameters", {})
    comment = data.get("comment", "")
    
    # Yeni versiyon adÄ±nÄ± belirle
    new_version = get_next_version(model)
    
    # Model datasÄ±nÄ± al
    model_data = get_model_data(model)
    
    # Cluster config oluÅŸtur
    cluster_config = {
        "model_type": model,
        "version": new_version,
        "algorithm": algorithm,
        **parameters
    }
    
    # Config'i kaydet
    with open("cluster_config.json", "w", encoding="utf-8") as f:
        json.dump(cluster_config, f, indent=2, ensure_ascii=False)
    
    try:
        # Versiyon klasÃ¶rÃ¼nÃ¼ oluÅŸtur
        version_path = os.path.join("exported_clusters", model, new_version)
        thumbnail_path = os.path.join(version_path, "thumbnails")
        os.makedirs(thumbnail_path, exist_ok=True)
        
        # Auto cluster'Ä± Ã§alÄ±ÅŸtÄ±r
        python_exe = sys.executable
        env = os.environ.copy()
        env["PYTHONIOENCODING"] = "utf-8"
        
        result = subprocess.run(
            [python_exe, "auto_cluster.py"], 
            capture_output=True, 
            text=True, 
            check=True, 
            env=env
        )
        
        # Ã‡Ä±ktÄ±dan cluster bilgilerini al
        cluster_count = 0
        for line in result.stdout.splitlines():
            if "kÃ¼me oluÅŸturuldu" in line:
                parts = line.split()
                try:
                    cluster_count = int(parts[parts.index("kÃ¼me") - 1])
                except (ValueError, IndexError):
                    pass
        
        # GÃ¼ncel verileri al
        updated_model_data = get_model_data(model)
        updated_version_data = updated_model_data["versions"].get(new_version, {})
        actual_clusters = updated_version_data.get("clusters", {})
        actual_cluster_count = len(actual_clusters)
        
        return jsonify({
            "status": "ok", 
            "version": new_version,
            "cluster_count": actual_cluster_count or cluster_count,
            "message": f"{model} modeli iÃ§in {new_version} versiyonu oluÅŸturuldu."
        })
    
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)})

@app.route("/available-versions")
def available_versions():
    """Bir model iÃ§in mevcut tÃ¼m versiyonlarÄ± listeler"""
    model = request.args.get("model", "pattern")
    
    try:
        model_data = get_model_data(model)
        versions = []
        
        for version_id in model_data["versions"].keys():
            version_data = model_data["versions"][version_id]
            version_name = f"Versiyon {version_id[1:]}" if version_id.startswith("v") else version_id
            
            # Ä°steÄŸe baÄŸlÄ± olarak versiyon hakkÄ±nda daha fazla bilgi eklenebilir
            versions.append({
                "id": version_id,
                "name": version_name,
                "algorithm": version_data.get("algorithm", "unknown"),
                "created_at": version_data.get("created_at", ""),
                "comment": version_data.get("comment", "")
            })
        
        # EÄŸer liste boÅŸsa varsayÄ±lan ekle
        if not versions:
            versions.append({"id": "v1", "name": "Versiyon 1"})
            
        return jsonify(versions)
    except Exception as e:
        print(f"Versiyon listesi alÄ±nÄ±rken hata: {str(e)}")
        return jsonify([{"id": "v1", "name": "Versiyon 1"}])

# Yeni: DeÄŸiÅŸiklikleri uygulama endpoint'i
@app.route("/apply-updates", methods=["POST"])
def apply_updates():
    try:
        from check_updates import apply_updates, check_for_updates
        
        # Ä°lk Ã¶nce deÄŸiÅŸiklikleri kontrol et
        changes = check_for_updates()
        
        if not changes.get("has_changes", False):
            return jsonify({
                "status": "no_changes", 
                "message": "Uygulanacak deÄŸiÅŸiklik bulunamadÄ±."
            })
        
        # Ä°ÅŸlem zaten devam ediyorsa bildir
        if os.path.exists('updating_clusters.flag'):
            flag_age = time.time() - os.path.getmtime('updating_clusters.flag')
            return jsonify({
                "status": "in_progress",
                "message": f"â³ BaÅŸka bir gÃ¼ncelleme iÅŸlemi {flag_age/60:.1f} dakikadÄ±r devam ediyor, lÃ¼tfen bekleyin."
            })
            
        # DeÄŸiÅŸiklikleri uygula
        results = apply_updates(changes)
        
        # Faiss indekslerini temizle
        global faiss_indexes, cached_features, cached_filenames, cached_metadata
        faiss_indexes = {}
        cached_features = {}
        cached_filenames = {}
        cached_metadata = None
        
        return jsonify({
            "status": results.get("status", "success"),
            "message": "âœ… DeÄŸiÅŸiklikler baÅŸarÄ±yla uygulandÄ±" if results.get("status") == "success" else "âš ï¸ BazÄ± deÄŸiÅŸiklikler uygulanamadÄ±",
            "results": results
        })
    except Exception as ex:
        traceback.print_exc()
        return jsonify({
            "status": "error",
            "message": f"âŒ Hata oluÅŸtu: {str(ex)}"
        })

# Yeni: DeÄŸiÅŸiklikleri yoksay endpoint'i
@app.route("/ignore-updates", methods=["POST"])
def ignore_updates():
    try:
        data = request.get_json()
        previous_cache = data.get("previous_cache", {})
        
        # Ã–nceki cache durumunu kaydet (deÄŸiÅŸiklikleri yoksaymak iÃ§in)
        if previous_cache:
            from check_updates import save_current_cache
            save_current_cache(
                previous_cache.get("images", []), 
                previous_cache.get("json_hashes", {})
            )
            return jsonify({"status": "ok", "message": "DeÄŸiÅŸiklikler yoksayÄ±ldÄ±."})
        else:
            return jsonify({"status": "error", "message": "Ã–nceki cache bilgisi bulunamadÄ±."})
    except Exception as ex:
        traceback.print_exc()
        return jsonify({"status": "error", "message": f"âŒ Hata oluÅŸtu: {str(ex)}"})

# Yeni: Sistem bilgilerini dÃ¶ndÃ¼r
@app.route("/system-info")
def system_info():
    # Faiss GPU desteÄŸi kontrolÃ¼
    gpu_count = faiss.get_num_gpus()
    uses_gpu = gpu_count > 0
    
    # Mevcut model tipleri ve versiyonlarÄ±
    models = []
    for model_type in ["pattern", "color", "texture", "color+pattern"]:
        model_path = os.path.join("exported_clusters", model_type, "versions.json")
        if os.path.exists(model_path):
            try:
                with open(model_path, "r", encoding="utf-8") as f:
                    model_data = json.load(f)
                
                model_info = {
                    "model_type": model_type,
                    "current_version": model_data.get("current_version", "v1"),
                    "versions": list(model_data.get("versions", {}).keys())
                }
                models.append(model_info)
            except:
                pass
    
    # Model Ã¶zellikleri
    features_info = []
    for model_type in ["pattern", "color", "texture", "color+pattern"]:
        feature_path = f"image_features/{model_type}_features.npy"
        if os.path.exists(feature_path):
            try:
                features = np.load(feature_path)
                features_info.append({
                    "model_type": model_type,
                    "shape": list(features.shape),
                    "size_mb": round(os.path.getsize(feature_path) / (1024 * 1024), 2)
                })
            except:
                pass
    
    # GÃ¶rsel sayÄ±sÄ±
    image_count = len([f for f in os.listdir("realImages") if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
    
    return jsonify({
        "app_version": APP_VERSION,
        "last_updated": APP_LAST_UPDATED,
        "faiss": {
            "gpu_count": gpu_count,
            "uses_gpu": uses_gpu
        },
        "models": models,
        "features": features_info,
        "image_count": image_count,
        "environment": {
            "python_version": sys.version.split()[0]
        }
    })

# Yeni: Uyumlu renklere sahip kumaÅŸlarÄ± bulma endpoint'i
@app.route("/find-harmonious")
def find_harmonious():
    fabric_id = request.args.get("fabricId")
    harmony_type = request.args.get("harmonyType", "complementary")
    threshold = float(request.args.get("threshold", "80")) / 100  # YÃ¼zdeyi ondalikli sayÄ±ya Ã§evir
    result_count = int(request.args.get("resultCount", "20"))
    current_season_only = request.args.get("currentSeasonOnly", "false").lower() == "true"
    current_quality_only = request.args.get("currentQualityOnly", "false").lower() == "true"
    
    # Parametreleri kontrol et
    if not fabric_id:
        return jsonify({"status": "error", "message": "fabricId parametresi gerekli."})
        
    if harmony_type not in ["complementary", "analogous", "triadic", "split_complementary", "monochromatic"]:
        harmony_type = "complementary"  # VarsayÄ±lan olarak tamamlayÄ±cÄ± renk kullan
    
    # SeÃ§ilen kumaÅŸÄ±n dosya yolu
    img_path = os.path.join("realImages", fabric_id)
    if not os.path.exists(img_path):
        return jsonify({"status": "error", "message": f"GÃ¶rsel dosyasÄ± bulunamadÄ±: {fabric_id}"})
    
    # Metadata dosyasÄ±nÄ± kontrol et
    if not os.path.exists("image_metadata_map.json"):
        return jsonify({"status": "error", "message": "Metadata dosyasÄ± bulunamadÄ±."})
    
    # Metadata yÃ¼kle
    with open("image_metadata_map.json", "r", encoding="utf-8") as f:
        metadata = json.load(f)
    
    # KumaÅŸÄ±n 1x1 indirgenerek dominant renk bilgisini al
    img = cv2.imread(img_path)
    if img is None:
        return jsonify({"status": "error", "message": "GÃ¶rsel dosyasÄ± okunamadÄ±."})
    
    # RGB dominant rengi al (BGR'den RGB'ye dÃ¶nÃ¼ÅŸtÃ¼r)
    dominant_color = cv2.resize(img, (1, 1))[0, 0].tolist()  # [B, G, R] formatÄ±nda
    dominant_color = dominant_color[::-1]  # [R, G, B] formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
    
    # Uyumlu renkleri bul
    harmonious_colors = find_harmonious_colors(dominant_color, harmony_type)
    
    # Referans kumaÅŸÄ±n metadata bilgisini al (filtreleme iÃ§in)
    reference_metadata = metadata.get(fabric_id, {})
    reference_season = reference_metadata.get("season")
    reference_quality = reference_metadata.get("quality")
    
    # DÃ¶ndÃ¼rÃ¼lecek sonuÃ§larÄ± hazÄ±rla
    results = []
    found_count = 0
    
    # TÃ¼m realImages klasÃ¶rÃ¼ndeki kumaÅŸlarÄ± tara
    all_fabrics = [f for f in os.listdir("realImages") if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    for fabric_name in all_fabrics:
        if fabric_name == fabric_id:
            continue  # Kendisini dÄ±ÅŸarÄ±da bÄ±rak
        
        # Bu kumaÅŸÄ±n metadata bilgisini al (eÄŸer varsa)
        fabric_metadata = metadata.get(fabric_name, {})
        
        # Filtreleme seÃ§enekleri kontrol
        if current_season_only and reference_season and fabric_metadata.get("season") != reference_season:
            continue  # Mevsim eÅŸleÅŸmezse atla
            
        if current_quality_only and reference_quality and fabric_metadata.get("quality") != reference_quality:
            continue  # Kalite eÅŸleÅŸmezse atla
        
        fabric_img_path = os.path.join("realImages", fabric_name)
        fabric_img = cv2.imread(fabric_img_path)
        if fabric_img is None:
            continue
        
        # KumaÅŸÄ±n dominant rengini al (BGR'den RGB'ye dÃ¶nÃ¼ÅŸtÃ¼r)
        fabric_dominant_color = cv2.resize(fabric_img, (1, 1))[0, 0].tolist()[::-1]  # [R, G, B]
        
        # Bu kumaÅŸÄ±n rengi, uyumlu renklerden herhangi birine ne kadar benzer?
        best_similarity = 0
        best_harmony_color = None
        
        for harmonious_color in harmonious_colors:
            similarity = calculate_color_similarity(harmonious_color, fabric_dominant_color)
            if similarity > best_similarity:
                best_similarity = similarity
                best_harmony_color = harmonious_color
        
        # Benzerlik belirli bir eÅŸiÄŸin Ã¼zerindeyse, sonuÃ§lara ekle
        if best_similarity > threshold:
            results.append({
                "filename": fabric_name,
                "similarity": round(float(best_similarity), 3),
                "harmony_type": harmony_type,
                "harmony_color": best_harmony_color,
                "dominant_color": fabric_dominant_color,
                "design": fabric_metadata.get("design"),
                "season": fabric_metadata.get("season"),
                "features": fabric_metadata.get("features", []),
                "quality": fabric_metadata.get("quality")
            })
            found_count += 1
            
            # SonuÃ§ sayÄ±sÄ± limiti aÅŸÄ±ldÄ±ysa dÃ¶ngÃ¼yÃ¼ sonlandÄ±r
            if found_count >= result_count:
                break
    
    # BenzerliÄŸe gÃ¶re sÄ±rala (en benzerden baÅŸlayarak)
    results = sorted(results, key=lambda x: x["similarity"], reverse=True)
    
    # SonuÃ§larÄ± dÃ¶ndÃ¼r
    return jsonify({
        "status": "success",
        "reference_fabric": fabric_id,
        "reference_color": dominant_color,
        "harmony_type": harmony_type,
        "harmony_colors": harmonious_colors,
        "found_count": found_count,
        "results": results
    })

# Yeni: Faiss indekslerini sÄ±fÄ±rlama endpoint'i
@app.route("/reset-faiss-indexes")
def reset_faiss_indexes():
    global faiss_indexes, cached_features, cached_filenames, cached_metadata
    faiss_indexes = {}
    cached_features = {}
    cached_filenames = {}
    cached_metadata = None
    return jsonify({"status": "ok", "message": "Faiss indeksleri sÄ±fÄ±rlandÄ±."})

# app.py baÅŸlatÄ±lÄ±rken Faiss indekslerinin durumunu kontrol et
def check_faiss_reset_flag():
    flag_file = "reset_faiss_indexes.flag"
    if os.path.exists(flag_file):
        # DosyayÄ± sil
        os.remove(flag_file)
        # Faiss indekslerini sÄ±fÄ±rla
        global faiss_indexes, cached_features, cached_filenames, cached_metadata
        faiss_indexes = {}
        cached_features = {}
        cached_filenames = {}
        cached_metadata = None
        print("ğŸ”„ Faiss indeksleri sÄ±fÄ±rlandÄ± (reset flag algÄ±landÄ±)")

# UygulamayÄ± baÅŸlatÄ±rken flag kontrolÃ¼
check_faiss_reset_flag()

if __name__ == "__main__":
    # Uygulama baÅŸlarken bir temizlik zamanlamasÄ± baÅŸlat
    clean_timer = threading.Timer(4 * 60 * 60, clean_faiss_indexes_periodically)  # 4 saat
    clean_timer.daemon = True
    clean_timer.start()
    
    app.run(debug=True, use_reloader=False)
