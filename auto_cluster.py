# auto_cluster.py
# OluÅŸturulma: 2025-04-19
# GÃ¼ncelleme: 2025-05-06 (Renk modeli iÃ§in Ã¶zel sÄ±nÄ±flandÄ±rma eklendi - FEATURE-009-D)
# HazÄ±rlayan: Kafkas
# AÃ§Ä±klama:
# Bu dosya, Power User arayÃ¼zÃ¼nden gelen parametrelere gÃ¶re otomatik gÃ¶rsel kÃ¼meleri oluÅŸturur.
# Desteklenen algoritmalar: KMeans, DBSCAN, Hierarchical
# Renk modeli iÃ§in Ã¶zel spektrum-bazlÄ± sÄ±nÄ±flandÄ±rma eklendi (color_cluster.py)
# GiriÅŸ verileri: image_features/{model_type}_features.npy ve corresponding image list
# Ã‡Ä±ktÄ±: exported_clusters/{model_type}/{version}/thumbnails/{image.jpg}
# versions.json gÃ¼ncellenir
# Faiss kullanarak hÄ±zlandÄ±rÄ±lmÄ±ÅŸ kÃ¼meleme iÅŸlemleri

import os
import json
import numpy as np
import shutil
import time
from datetime import datetime
from sklearn.cluster import DBSCAN, AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_distances
from sklearn.preprocessing import StandardScaler
import faiss

# Renk modeli iÃ§in Ã¶zel sÄ±nÄ±flandÄ±rma modÃ¼lÃ¼
from color_cluster import cluster_images as color_cluster_method

# YardÄ±mcÄ± fonksiyonlar
def get_model_data(model_type):
    """Model JSON verisini okur"""
    model_path = os.path.join("exported_clusters", model_type, "versions.json")
    
    if not os.path.exists(model_path):
        # Yeni model dosyasÄ± oluÅŸtur
        data = {
            "current_version": "v1",
            "versions": {}
        }
        # Model klasÃ¶rÃ¼nÃ¼ oluÅŸtur
        os.makedirs(os.path.join("exported_clusters", model_type), exist_ok=True)
        with open(model_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        return data
        
    try:
        with open(model_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError:
        # Bozuksa yeniden oluÅŸtur
        return {
            "current_version": "v1",
            "versions": {}
        }

def save_model_data(model_type, data):
    """Model JSON verisini kaydeder"""
    model_path = os.path.join("exported_clusters", model_type, "versions.json")
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    
    with open(model_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    return True

def main(config_path="cluster_config.json"):
    """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu"""
    # 1. AyarlarÄ± oku
    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)

    model_type = config.get("model_type")
    version = config.get("version", "v1")  # Versiyon parametresi eklendi
    algorithm = config.get("algorithm")
    
    # ğŸ¨ Renk modeli iÃ§in Ã¶zel sÄ±nÄ±flandÄ±rma kullan
    if model_type == "color":
        print(f"ğŸ¨ Renk modeli tespit edildi. Ã–zel renk spektrumu sÄ±nÄ±flandÄ±rmasÄ± kullanÄ±lÄ±yor...")
        return color_cluster_method(config)
        
    # Standart kÃ¼meleme algoritmalarÄ± iÃ§in normal akÄ±ÅŸa devam et

    # Parametreler (opsiyonel olabilir)
    k = int(config.get("k", 5))
    eps = float(config.get("eps", 0.5))
    min_samples = int(config.get("min_samples", 5))
    linkage = config.get("linkage", "ward")

    # ğŸ” Model tÃ¼rÃ¼ne gÃ¶re benzerlik metriÄŸini belirle
    if model_type == "pattern":
        distance_metric = "cosine"
    elif model_type == "color":
        distance_metric = "euclidean"
    elif model_type == "texture":
        distance_metric = "manhattan"
    elif model_type == "color+pattern":
        distance_metric = "cosine"  # Renk+Desen iÃ§in cosine similarity kullan
    else:
        distance_metric = "euclidean"  # varsayÄ±lan

    feature_path = f"image_features/{model_type}_features.npy"
    image_list_path = f"image_features/{model_type}_filenames.json"

    # 2. VektÃ¶rleri ve gÃ¶rselleri yÃ¼kle
    if not os.path.exists(feature_path):
        raise FileNotFoundError(f"Feature dosyasÄ± bulunamadÄ±: {feature_path}")

    X = np.load(feature_path)
    with open(image_list_path, "r", encoding="utf-8") as f:
        image_paths = json.load(f)

    X = StandardScaler().fit_transform(X)

    # 3. Clustering algoritmasÄ±nÄ± Ã§alÄ±ÅŸtÄ±r
    start_time = time.time()

    # Verileri float32'ye dÃ¶nÃ¼ÅŸtÃ¼r (Faiss iÃ§in daha verimli)
    X = X.astype(np.float32)

    if algorithm == "kmeans":
        # GPU kontrolÃ¼
        use_gpu = faiss.get_num_gpus() > 0
        d = X.shape[1]  # Ã¶zellik vektÃ¶rÃ¼ boyutu
        
        print(f"ğŸ” KMeans kÃ¼meleme baÅŸlatÄ±lÄ±yor: k={k}, vektÃ¶r boyutu={d}")
        
        if use_gpu:
            print("ğŸš€ GPU kullanÄ±lÄ±yor")
            # GPU KMeans
            kmeans = faiss.Kmeans(d, k, niter=300, verbose=True, gpu=True)
        else:
            print("ğŸ’» CPU kullanÄ±lÄ±yor")
            # CPU KMeans
            kmeans = faiss.Kmeans(d, k, niter=300, verbose=True)
        
        kmeans.train(X)
        centroids = kmeans.centroids
        _, labels = kmeans.index.search(X, 1)
        labels = labels.reshape(-1)
        
        print(f"âœ… KMeans kÃ¼meleme tamamlandÄ±. {k} kÃ¼me oluÅŸturuldu.")
        
    elif algorithm == "dbscan":
        print(f"ğŸ” DBSCAN kÃ¼meleme baÅŸlatÄ±lÄ±yor: eps={eps}, min_samples={min_samples}")
        
        # Model tÃ¼rÃ¼ne gÃ¶re metrik seÃ§imi
        if distance_metric == "cosine":
            # KosinÃ¼s mesafesi iÃ§in vektÃ¶rleri normalleÅŸtir
            faiss.normalize_L2(X)
            clustering = DBSCAN(eps=eps, min_samples=min_samples, metric="euclidean")
        else:
            clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=distance_metric)
        
        labels = clustering.fit_predict(X)
        print(f"âœ… DBSCAN kÃ¼meleme tamamlandÄ±. {len(set(labels)) - (1 if -1 in labels else 0)} kÃ¼me oluÅŸturuldu.")
        
    elif algorithm == "hierarchical":
        print(f"ğŸ” Hierarchical kÃ¼meleme baÅŸlatÄ±lÄ±yor: k={k}, linkage={linkage}")
        clustering = AgglomerativeClustering(n_clusters=k, linkage=linkage)
        labels = clustering.fit_predict(X)
        print(f"âœ… Hierarchical kÃ¼meleme tamamlandÄ±. {k} kÃ¼me oluÅŸturuldu.")
    else:
        raise ValueError(f"Bilinmeyen algoritma: {algorithm}")

    # Performans Ã¶lÃ§Ã¼mÃ¼
    end_time = time.time()
    duration = end_time - start_time
    print(f"â±ï¸ KÃ¼meleme sÃ¼resi: {duration:.2f} saniye")

    # 4. SonuÃ§larÄ± model verilerine ekle
    output_dir = os.path.join("exported_clusters", model_type, version)  # Versiyon eklendi
    os.makedirs(output_dir, exist_ok=True)  # Ana klasÃ¶rÃ¼ oluÅŸtur

    # Thumbnail klasÃ¶rÃ¼nÃ¼ oluÅŸtur
    thumbnail_dir = os.path.join(output_dir, "thumbnails")
    os.makedirs(thumbnail_dir, exist_ok=True)

    # Model datasÄ±nÄ± al
    model_data = get_model_data(model_type)

    # Versiyon yoksa oluÅŸtur
    if version not in model_data["versions"]:
        model_data["versions"][version] = {
            "created_at": datetime.now().isoformat(),
            "comment": f"{model_type} modeli {version} versiyonu - {algorithm} ile oluÅŸturuldu",
            "algorithm": algorithm,
            "parameters": {
                "k": k if algorithm == "kmeans" else None,
                "eps": eps if algorithm == "dbscan" else None,
                "min_samples": min_samples if algorithm == "dbscan" else None,
                "linkage": linkage if algorithm == "hierarchical" else None
            },
            "clusters": {}
        }

    # Mevcut version verisi
    version_data = model_data["versions"][version]

    # Ã–nceki cluster'larÄ± temizle (eÄŸer varsa)
    version_data["clusters"] = {}

    # DetaylÄ± bir log dosyasÄ± oluÅŸtur
    log_path = os.path.join(output_dir, "cluster_log.txt")
    with open(log_path, "w", encoding="utf-8") as log_file:
        log_file.write(f"Algoritma: {algorithm}\n")
        total_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        log_file.write(f"Toplam kÃ¼me sayÄ±sÄ±: {total_clusters}\n")
        log_file.write(f"KÃ¼meleme sÃ¼resi: {duration:.2f} saniye\n")
        
        # KÃ¼me boyutlarÄ± analizi
        log_file.write("\nKÃ¼me boyutlarÄ±:\n")
        label_counts = {}
        for label in labels:
            if label == -1:  # DBSCAN outlier'larÄ±
                continue
            label_counts[label] = label_counts.get(label, 0) + 1
        
        for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):
            log_file.write(f"  KÃ¼me {label}: {count} gÃ¶rsel\n")
        
        # Outlier sayÄ±sÄ± (sadece DBSCAN iÃ§in)
        if algorithm == "dbscan":
            outlier_count = np.sum(labels == -1)
            log_file.write(f"\nOutlier sayÄ±sÄ±: {outlier_count}\n")

    # GÃ¶rselleri iÅŸle ve cluster'lara ekle
    start_process_time = time.time()
    processed_count = 0

    for idx, label in enumerate(labels):
        if label == -1:
            if algorithm == "dbscan":
                # DBSCAN outlier'larÄ± iÃ§in Ã¶zel bir kÃ¼me oluÅŸtur (opsiyonel)
                cluster_name = "outliers"
                # Outlier'lar iÃ§in bir kÃ¼me oluÅŸtur (yoksa)
                if cluster_name not in version_data["clusters"]:
                    version_data["clusters"][cluster_name] = {
                        "representative": "",  # Ä°lk outlier gÃ¶rseli eklendiÄŸinde gÃ¼ncellenecek
                        "images": [],
                        "comment": "DBSCAN tarafÄ±ndan outlier olarak iÅŸaretlenen gÃ¶rseller"
                    }
            else:
                continue  # DiÄŸer algoritmalar iÃ§in outlier konsepti yok, atla
        else:
            # Normal kÃ¼me adÄ±
            cluster_name = f"cluster-{label}"
        
        # Kaynak dosya bilgileri
        filename = os.path.basename(image_paths[idx])
        
        # EÄŸer normal kÃ¼me yoksa oluÅŸtur
        if cluster_name not in version_data["clusters"]:
            version_data["clusters"][cluster_name] = {
                "representative": filename,
                "images": [],
                "comment": ""
            }
        elif cluster_name == "outliers" and not version_data["clusters"][cluster_name]["representative"]:
            # Outlier kÃ¼mesi iÃ§in ilk gÃ¶rseli temsilci olarak ayarla
            version_data["clusters"][cluster_name]["representative"] = filename
        
        # GÃ¶rseli kÃ¼meye ekle
        if filename not in version_data["clusters"][cluster_name]["images"]:
            version_data["clusters"][cluster_name]["images"].append(filename)
            processed_count += 1
        
        # Thumbnail'i kopyala
        src_thumb = os.path.join("thumbnails", filename)
        dst_thumb = os.path.join(thumbnail_dir, filename)
        
        if os.path.exists(src_thumb):
            shutil.copy2(src_thumb, dst_thumb)

    process_duration = time.time() - start_process_time
    print(f"â±ï¸ GÃ¶rsel iÅŸleme sÃ¼resi: {process_duration:.2f} saniye ({processed_count} gÃ¶rsel iÅŸlendi)")

    # Metadata update - kÄ±sa yoldan
    if os.path.exists("image_metadata_map.json"):
        try:
            with open("image_metadata_map.json", "r", encoding="utf-8") as f:
                metadata = json.load(f)
            
            # Her clusterdaki gÃ¶rsellerin metadata'sÄ±nÄ± gÃ¼ncelle
            for cluster_name, cluster_data in version_data["clusters"].items():
                for img in cluster_data["images"]:
                    if img in metadata:
                        metadata[img]["cluster"] = cluster_name
            
            # Metadata'yÄ± kaydet
            with open("image_metadata_map.json", "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
        except Exception:
            pass

    # GÃ¼ncel versiyonu ayarla
    model_data["current_version"] = version

    # Model verisini kaydet
    save_model_data(model_type, model_data)

    total_clusters = len(version_data['clusters'])
    total_images = sum(len(cluster['images']) for cluster in version_data['clusters'].values())
    total_duration = time.time() - start_time

    print(f"âœ… Otomatik kÃ¼meleme tamamlandÄ±.")
    print(f"   - {total_clusters} kÃ¼me oluÅŸturuldu")
    print(f"   - {total_images} gÃ¶rsel kÃ¼melere atandÄ±")
    print(f"   - Toplam sÃ¼re: {total_duration:.2f} saniye")

    # Log dosyasÄ±na toplam sÃ¼reyi ekle
    with open(log_path, "a", encoding="utf-8") as log_file:
        log_file.write(f"\nToplam iÅŸlem sÃ¼resi: {total_duration:.2f} saniye\n")
        log_file.write(f"Toplam kÃ¼me sayÄ±sÄ±: {total_clusters}\n")
        log_file.write(f"Toplam iÅŸlenen gÃ¶rsel: {total_images}\n")
        
    return version_data

if __name__ == "__main__":
    # KonfigÃ¼rasyon dosyasÄ±ndan Ã§alÄ±ÅŸtÄ±r
    main()
